{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_TRAIN = Path('./data/jigsaw_toxic_comments/train.csv')\n",
    "IN_TEST = Path('./data/jigsaw_toxic_comments/merged_test.csv')\n",
    "\n",
    "MODEL_OUT_VA_DIR = Path('./data/model_outputs/stack/')\n",
    "OUT_VA_CNN = MODEL_OUT_VA_DIR / 'pred_cnn.csv'\n",
    "OUT_VA_LSTM = MODEL_OUT_VA_DIR / 'pred_lstm.csv'\n",
    "OUT_VA_LOGREG = MODEL_OUT_VA_DIR / 'pred_logreg.csv'\n",
    "OUT_VA_NB_SVM = MODEL_OUT_VA_DIR / 'pred_nb_svm.csv'\n",
    "# OUT_VA_BERT = MODEL_OUTPUT_VA_DIR / 'pred_bert.csv'\n",
    "\n",
    "MODEL_OUT_DIR = Path('./data/model_outputs/truncated/')\n",
    "OUT_CNN = MODEL_OUT_DIR / 'pred_cnn.csv'\n",
    "OUT_LSTM = MODEL_OUT_DIR / 'pred_lstm.csv'\n",
    "OUT_LOGREG = MODEL_OUT_DIR / 'pred_logreg.csv'\n",
    "OUT_NB_SVM = MODEL_OUT_DIR / 'pred_nb_svm.csv'\n",
    "# OUT_BERT = MODEL_OUT_DIR / 'pred_bert.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = pd.read_csv(IN_TRAIN)\n",
    "TEST = pd.read_csv(IN_TEST)\n",
    "TRAIN, VALID = TRAIN[TRAIN.index <= 100_000], TRAIN[TRAIN.index > 100_000]\n",
    "\n",
    "LABEL_COLS = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "MODEL_OUT_VA_PROB = {\n",
    "    'CNN':    OUT_VA_CNN, \n",
    "    'LSTM':   OUT_VA_LSTM, \n",
    "    'LOGREG': OUT_VA_LOGREG, \n",
    "    'NB_SVM': OUT_VA_NB_SVM,\n",
    "    # 'BERT':   OUT_VA_BERT\n",
    "}\n",
    "\n",
    "MODEL_OUT_PROB = {\n",
    "    'CNN':    OUT_CNN, \n",
    "    'LSTM':   OUT_LSTM, \n",
    "    'LOGREG': OUT_LOGREG, \n",
    "    'NB_SVM': OUT_NB_SVM,\n",
    "    # 'BERT':   OUT_BERT\n",
    "}\n",
    "\n",
    "MODEL_OUT_VA_PROB = {\n",
    "    k: pd.read_csv(v)[LABEL_COLS].to_numpy() for k, v in MODEL_OUT_VA_PROB.items()\n",
    "}\n",
    "\n",
    "MODEL_OUT_PROB = {\n",
    "    k: pd.read_csv(v)[LABEL_COLS].to_numpy() for k, v in MODEL_OUT_PROB.items()\n",
    "}\n",
    "\n",
    "N_TE = len(TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_pred = np.zeros((len(TEST), len(LABEL_COLS)))\n",
    "for c_idx, col in enumerate(LABEL_COLS):\n",
    "    train_stack = np.column_stack([\n",
    "        model[:, c_idx] for model in MODEL_OUT_VA_PROB.values()\n",
    "    ])\n",
    " \n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=10,\n",
    "        max_leaf_nodes=4\n",
    "    )\n",
    "    clf = clf.fit(train_stack, VALID[col].to_numpy())\n",
    "\n",
    "    test_stack = np.column_stack([\n",
    "        model[:, c_idx] for model in MODEL_OUT_PROB.values()\n",
    "    ])\n",
    "    stacked_pred[:, c_idx] = clf.predict_proba(test_stack)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Model    |   AUC Score |\n",
      "|:---------|------------:|\n",
      "| CNN      |     95.2601 |\n",
      "| LSTM     |     97.2482 |\n",
      "| LOGREG   |     97.9942 |\n",
      "| NB_SVM   |     97.6329 |\n",
      "| ENSEMBLE |     97.4374 |\n"
     ]
    }
   ],
   "source": [
    "def compute_test_auc(pred):\n",
    "    return roc_auc_score(TEST[LABEL_COLS].values, pred)\n",
    "\n",
    "scores = pd.DataFrame(columns=['Model', 'AUC Score'])\n",
    "scores.set_index('Model', inplace=True)\n",
    "\n",
    "for k, v in MODEL_OUT_PROB.items():\n",
    "    scores.loc[k] = [compute_test_auc(v)]\n",
    "\n",
    "scores.loc['ENSEMBLE'] = [compute_test_auc(stacked_pred)]\n",
    "\n",
    "scores *= 100\n",
    "print(scores.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
