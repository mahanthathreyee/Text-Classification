\section{Introduction}

The digital landscape offers a platform for the exchange of ideas, yet it is often marred by the presence of toxic and harmful content. This reality poses a significant challenge, as online abuse and harassment can silence voices and hinder the richness of diverse dialogues. In response, online platforms frequently resort to restrictive measures, such as limiting or completely removing user comments, to maintain civility.

Addressing this challenge, our report presents an in-depth exploration of advanced classification models aimed at identifying and categorizing offensive online content. Our analysis focuses on six critical categories of toxicity: toxic, severe toxic, obscene, threat, insult, and identity hate. Utilizing a comprehensive dataset comprising 159,571 labeled and 153,164 unlabeled entries, we train and evaluate a variety of models, striving to foster a safer, more inclusive online environment.

Our exploration is structured into three distinct parts:
\begin{itemize}
    \item \textbf{Part 1: Deciphering the Data}: This section delves into the dataset's intricacies, discussing our preprocessing strategies and the text embedding techniques we employed to enhance the predictive power of our models.
    \item \textbf{Part 2: A Spectrum of Models}: Here, we embark on an exploration of various classification models. Each model presents a unique approach and set of strengths in addressing the issue of online toxicity.
    \item \textbf{Part 3: Insights and Horizons}: In this concluding section, we present the outcomes of our model evaluations, highlighting the models that most effectively identify harmful content. We conclude by discussing potential future directions, focusing on how these models can be refined and applied to cultivate a respectful and constructive online discourse.
\end{itemize}

Embark with us on this critical journey to balance the scales between open expression and the imperative to shield online communities from the adverse effects of toxic content.
